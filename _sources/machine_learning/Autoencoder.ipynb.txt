{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with Variational Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tfp.distributions\n",
    "\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* epoch: one cycle through training data (made up of many steps)\n",
    "* step: one gradient update per batch of data\n",
    "* learning rate: how fast to follow gradient (in gradient descent)\n",
    "\n",
    "* neural network: find function $y = f(x)$ with features $x$ and values/labels $y$ (regression/classification)\n",
    "* autoencoder:\n",
    "    * find function mapping $x$ to itself: $z = f(h_e(x))$, $\\hat{x} = f(h_d(z))$\n",
    "    * undercomplete: $dim(latent\\ space) < dim(input\\ space)$ (vs overcomplete)\n",
    "    * problem of capacity:\n",
    "        * if encoder is too powerful, one dimensional latent space could encode all training samples -> autoencoder fails to learn anything useful\n",
    "    * regularized: sophisticated loss-function (sparsity, robustness, ...)\n",
    "    * variational: enforce distribution for latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(2.)\n",
    "b = tf.constant(3.)\n",
    "\n",
    "print(tf.add(a, b))\n",
    "print(tf.reduce_mean([a, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = tf.constant([[1., 2.], [3., 4.]])\n",
    "m2 = tf.constant([[5., 6.], [7., 8.]])\n",
    "\n",
    "tf.matmul(m1, m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given data $X$, latent variable $z$, probability distribution of data $P(X)$, p.d. of latent variable $P(z)$, p.d. of generated data given latent variable $P(X|z)$\n",
    "\n",
    "Goal is to model data, i.e. find $P(X) = \\int P(X|z) P(z)$\n",
    "\n",
    "Idea of VAE:\n",
    "\n",
    "* infer $P(z)$ from $P(z|X)$ using variational inference (inference as optimization problem)\n",
    "* model $P(z|X)$ using $Q(z|X)$, $Q$ is typically Gaussian\n",
    "* components:\n",
    "    * encoder: $Q(z|X)$\n",
    "    * decoder: $P(X|z)$\n",
    "\n",
    "Measure difference between two distributions: Kullbackâ€“Leibler divergence\n",
    "\n",
    "Objective function:\n",
    "$$\n",
    "\\log P(X)-D_{K L}[Q(z | X) \\| P(z | X)]=E[\\log P(X | z)]-D_{K L}[Q(z | X) \\| P(z)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"Convert input to low-dimensional representation.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, original_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.network = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(*original_dim, 1)),\n",
    "\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=32, kernel_size=3, strides=(2, 2), activation=tf.nn.relu),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=64, kernel_size=3, strides=(2, 2), activation=tf.nn.relu),\n",
    "\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(units=2*latent_dim)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"Reconstruct input from low-dimensional representation.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, original_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.network = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "            tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n",
    "\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=64, kernel_size=3, strides=(2, 2), padding='SAME', activation=tf.nn.relu),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=32, kernel_size=3, strides=(2, 2), padding='SAME', activation=tf.nn.relu),\n",
    "\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=1, kernel_size=3, strides=(1, 1), padding='SAME'),\n",
    "        ])\n",
    "\n",
    "    def call(self, z):\n",
    "        return self.network(z)\n",
    "\n",
    "\n",
    "class Autoencoder(tf.keras.Model):\n",
    "    \"\"\"Connect all components to single model.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, original_dim):\n",
    "        \"\"\"Initialize everything.\"\"\"\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # setup architecture\n",
    "        self._encoder = Encoder(\n",
    "            latent_dim=self.latent_dim, original_dim=original_dim)\n",
    "        self._decoder = Decoder(\n",
    "            latent_dim=self.latent_dim, original_dim=original_dim)\n",
    "\n",
    "        # helpful stuff\n",
    "        self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Input -> latent distribution.\"\"\"\n",
    "        mean, logvar = tf.split(self._encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def _reparameterize(self, mean, logvar):\n",
    "        \"\"\"Trick which is needed for backpropagation.\"\"\"\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"Latent -> input space.\"\"\"\n",
    "        return self._decoder(z)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"Generate distribution from input and reconstruct using it.\"\"\"\n",
    "        mu, _ = tf.split(self._encoder(x), num_or_size_splits=2, axis=1)\n",
    "        # why do we only pass mu?\n",
    "        reconstructed = self.decode(mu)\n",
    "        return reconstructed\n",
    "\n",
    "    def log_normal_pdf(self, sample, mean, logvar, raxis=1):\n",
    "        log2pi = tf.math.log(2. * np.pi)\n",
    "        return tf.reduce_sum(\n",
    "            -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "            axis=raxis)\n",
    "\n",
    "    def compute_loss(self, x):\n",
    "        \"\"\" Compute loss.\n",
    "\n",
    "            Hmmmmm\n",
    "        \"\"\"\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self._reparameterize(mean, logvar)\n",
    "        x_logit = self.decode(z)\n",
    "\n",
    "        cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "        logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "        logpz = self.log_normal_pdf(z, 0., 0.)\n",
    "        logqz_x = self.log_normal_pdf(z, mean, logvar)\n",
    "\n",
    "        return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "    def compute_gradients(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.compute_loss(x)\n",
    "            self.train_loss(loss)\n",
    "\n",
    "            return tape.gradient(loss, self.trainable_variables)\n",
    "\n",
    "    @tf.function\n",
    "    def train(self, x, optimizer):\n",
    "        gradients = self.compute_gradients(x)\n",
    "        optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps)\n",
    "\n",
    "    def save(self, fname):\n",
    "        \"\"\"Save model.\n",
    "            https://www.tensorflow.org/alpha/guide/keras/saving_and_serializing#saving_subclassed_models\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
    "        self.save_weights(fname, save_format='tf')\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_file(cls, fname):\n",
    "        model = cls(latent_dim, original_dim)  # TODO: load parameters from file\n",
    "\n",
    "        # train model briefly to infer architecture\n",
    "        # TODO\n",
    "\n",
    "        model.load_weights(fname)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "learning_rate = 1e-3\n",
    "latent_dim = 10\n",
    "original_dim = (28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_features, training_labels), (test_features, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# normalize to range [0, 1]\n",
    "training_features = training_features / np.max(training_features)\n",
    "\n",
    "# flatten 2D images into 1D\n",
    "training_features = training_features.reshape(training_features.shape[0], *original_dim, 1).astype(np.float32)\n",
    "\n",
    "# prepare dataset\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(training_features).batch(batch_size)\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: improve loss storage\n",
    "fname = 'models/autoencoder'\n",
    "\n",
    "if os.path.exists(f'{fname}.index'):\n",
    "    autoencoder = Autoencoder.load_from_file(fname)\n",
    "    df_loss = pd.read_csv('models/autoencoder_loss.csv')\n",
    "else:\n",
    "    autoencoder = Autoencoder(latent_dim=latent_dim, original_dim=original_dim)\n",
    "    \n",
    "    # train\n",
    "    loss_list = []\n",
    "    for epoch in trange(epochs, desc='Epochs'):\n",
    "        for step, batch_features in enumerate(training_dataset):\n",
    "            autoencoder.train(batch_features, opt)\n",
    "\n",
    "        loss_list.append({\n",
    "            'epoch': epoch,\n",
    "            'loss': autoencoder.train_loss.result().numpy()\n",
    "        })\n",
    "    df_loss = pd.DataFrame(loss_list)\n",
    "\n",
    "    # save\n",
    "    autoencoder.save(fname)\n",
    "    df_loss.to_csv('models/autoencoder_loss.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(x='epoch', y='loss', data=df_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = tf.random.normal(shape=(4, latent_dim), mean=10)\n",
    "inp = np.asarray([[0.] * latent_dim])\n",
    "\n",
    "inp[0][4] = -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "fig, ax_arr = plt.subplots(N, N)\n",
    "\n",
    "for i, x in enumerate(np.linspace(-10, 10, N)):\n",
    "    for j, y in enumerate(np.linspace(-10, 10, N)):\n",
    "        ax = ax_arr[i, j]\n",
    "        inp[0][:2] = np.random.normal([x, y])\n",
    "\n",
    "        img = autoencoder.sample(inp).numpy()\n",
    "\n",
    "        ax.imshow(img[0,:,:,0], cmap='gray')\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check specific examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sub = training_features[:10]\n",
    "features_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = tf.reshape(\n",
    "    features_sub, (features_sub.shape[0], 28, 28))\n",
    "reconstructed = tf.reshape(\n",
    "    autoencoder(tf.constant(features_sub)),\n",
    "    (features_sub.shape[0], 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 16\n",
    "fig, ax_list = plt.subplots(nrows=4, ncols=4, figsize=(8, 4))\n",
    "\n",
    "for i, ax in enumerate(ax_list.ravel()):\n",
    "    idx = np.random.randint(0, original.shape[0])\n",
    "\n",
    "    img_orig = original.numpy()[idx]\n",
    "    img_recon = reconstructed.numpy()[idx]\n",
    "\n",
    "    img_concat = np.concatenate([img_orig, img_recon], axis=1)\n",
    "\n",
    "    ax.imshow(img_concat, cmap='gray')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_PCA(X, ndim=2):\n",
    "    pca = PCA(n_components=ndim)\n",
    "    pca.fit(X)\n",
    "    X_trans = pca.transform(X)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    return pd.DataFrame(X_trans, columns=[f'PC_{i}' for i in range(ndim)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_features = autoencoder.encode(training_features)\n",
    "latent_features_1d = np.concatenate(latent_features, axis=1)\n",
    "\n",
    "df_latent = do_PCA(latent_features_1d)\n",
    "df_latent['label'] = training_labels\n",
    "df_latent['space'] = 'latent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_1d = training_features.reshape(\n",
    "    training_features.shape[0],\n",
    "    training_features.shape[1]*training_features.shape[2])\n",
    "\n",
    "df_original = do_PCA(data_train_1d)\n",
    "df_original['label'] = training_labels\n",
    "df_original['space'] = 'original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = pd.concat([df_original, df_latent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df_pca, col='space', hue='label', height=8, legend_out=True)\n",
    "g.map_dataframe(sns.scatterplot, x='PC_0', y='PC_1', rasterized=True)\n",
    "g.add_legend()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
